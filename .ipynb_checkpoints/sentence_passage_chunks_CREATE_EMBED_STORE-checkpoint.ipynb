{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85f8e313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emilh/anaconda3/envs/ou/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os, sys, contextlib\n",
    "from txtai.embeddings import Embeddings\n",
    "from txtai.pipeline import Extractor\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document \n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('punkt_tab')\n",
    "from nltk import tokenize\n",
    "from langchain import FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3666a1",
   "metadata": {},
   "source": [
    "# Guide to Sentence and Passage Chunking\n",
    "- This guide shows how sentence and passage chunking was implemented using nltk and langchain.text_splitter. It further covers embedding of the chunks using SentenceTransformers and LangChain Documents and saving them to a local vector database file using FAISS.\n",
    "- We use a subset of Yelp restaurant reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087584f1-25d9-4dfb-9288-df5e76e7e331",
   "metadata": {},
   "source": [
    "### Read a subset of YELP restaurant review data (20k reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da6c61fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=\"YELP\"\n",
    "data_path = 'data/YELP/yelp_subset.pkl'\n",
    "# Load a DataFrame of a subset of 20k YELP restaurant reviews from a pickle file\n",
    "df_reviews = pd.read_pickle(data_path)\n",
    "# Reset the index and rename the index column to \"Doc Id\"\n",
    "df_reviews.reset_index(inplace=True)\n",
    "df_reviews.rename(columns={'index': 'Doc Id'}, inplace=True)\n",
    "# Rename the column from 'text' to 'Doc Text'\n",
    "df_reviews.rename(columns={'text': 'Doc Text'}, inplace=True)\n",
    "columns_to_keep = ['Doc Id', 'review_id',\"business_id\",\"stars\",\"Doc Text\"]\n",
    "# Keep only the columns in the list\n",
    "df_reviews = df_reviews[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d34644d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>Doc Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112388</td>\n",
       "      <td>vhETeXa3nM34Hwk3KEFfiA</td>\n",
       "      <td>AQw0B8j9QV1RkFLLFiwkuw</td>\n",
       "      <td>3.0</td>\n",
       "      <td>I will be spending several weekends here in Ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68092</td>\n",
       "      <td>M09LOjNR1ymX4avcBQfAYQ</td>\n",
       "      <td>rh6O8NtKJUhqZ0G2Pkpj2Q</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Went here once and can't wait to go again! The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40901</td>\n",
       "      <td>w5x1pXvmODU5cYI3PZsSQA</td>\n",
       "      <td>YGgGefpPTFhgthvQvMAGoQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Now I know why Guy featured this place, it was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19599</td>\n",
       "      <td>LnbFwaD8CEC-OsCMb1YZDA</td>\n",
       "      <td>SZU9c8V2GuREDN5KgyHFJw</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Great place at the end of the wharf. Be prepar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>144853</td>\n",
       "      <td>3ZiPH6CHL_cyVNoYP2rt1Q</td>\n",
       "      <td>FQxEfhBd1gMrurP19bhK8w</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Mmm...I always get the chicken salad sandwich,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Doc Id               review_id             business_id  stars  \\\n",
       "0  112388  vhETeXa3nM34Hwk3KEFfiA  AQw0B8j9QV1RkFLLFiwkuw    3.0   \n",
       "1   68092  M09LOjNR1ymX4avcBQfAYQ  rh6O8NtKJUhqZ0G2Pkpj2Q    5.0   \n",
       "2   40901  w5x1pXvmODU5cYI3PZsSQA  YGgGefpPTFhgthvQvMAGoQ    5.0   \n",
       "3   19599  LnbFwaD8CEC-OsCMb1YZDA  SZU9c8V2GuREDN5KgyHFJw    4.0   \n",
       "4  144853  3ZiPH6CHL_cyVNoYP2rt1Q  FQxEfhBd1gMrurP19bhK8w    4.0   \n",
       "\n",
       "                                            Doc Text  \n",
       "0  I will be spending several weekends here in Ca...  \n",
       "1  Went here once and can't wait to go again! The...  \n",
       "2  Now I know why Guy featured this place, it was...  \n",
       "3  Great place at the end of the wharf. Be prepar...  \n",
       "4  Mmm...I always get the chicken salad sandwich,...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc47d7aa",
   "metadata": {},
   "source": [
    "# Select chunking strategy\n",
    " 1. passage_chunking\n",
    " 2. sentence_chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80580c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunking_strategy= \"passage_chunking\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3325db2",
   "metadata": {},
   "source": [
    "### Functions for chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56058c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def passage_chunking(df_reviews, size, overlap):\n",
    "    \"\"\"\n",
    "    Function to chunk review documents into smaller passages by character count, \n",
    "    with a specified chunk size and overlap to retain semantic context.\n",
    "\n",
    "    Inputs:\n",
    "        - df_reviews: DataFrame containing reviews.\n",
    "        - size: Integer specifying the size of each chunk in characters.\n",
    "        - overlap: Integer specifying the overlap between chunks in characters.\n",
    "\n",
    "    Outputs:\n",
    "        - A list of chunked documents where each chunk is represented as a Document object \n",
    "          with its associated metadata (e.g., document ID).\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize a list to store Document objects created from the input DataFrame.\n",
    "    docs = []\n",
    "\n",
    "    # Iterate through each row in the input DataFrame to process the reviews.\n",
    "    for index, row in df_reviews.iterrows():\n",
    "        # Extract the review text from the \"Doc Text\" column.\n",
    "        doc_text = row[\"Doc Text\"]\n",
    "\n",
    "        # Create metadata for the document using the \"Doc Id\" column.\n",
    "        doc_id = {\"review_id\": row[\"Doc Id\"]}\n",
    "\n",
    "        # Create a new Document object with the review text and metadata.\n",
    "        newDoc = Document(page_content=doc_text, metadata=doc_id)\n",
    "\n",
    "        # Append the Document object to the docs list.\n",
    "        docs.append(newDoc)\n",
    "\n",
    "    # Instantiate the RecursiveCharacterTextSplitter for chunking documents.\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=size,          # Set the size of each chunk in characters.\n",
    "        chunk_overlap=overlap,    # Set the overlap size between chunks in characters.\n",
    "        length_function=len,      # Define the function to measure text length.\n",
    "        is_separator_regex=False  # Specify whether the separator is a regex.\n",
    "    )\n",
    "\n",
    "    # Use the text splitter to split the documents into smaller chunks.\n",
    "    doc_chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "    # Return the list of chunked documents.\n",
    "    return doc_chunks\n",
    "\n",
    "def sentence_chunking(df_reviews):\n",
    "    \"\"\"\n",
    "    Function to chunk review documents into sentences. Each sentence becomes a separate document \n",
    "    with its associated metadata.\n",
    "\n",
    "    Inputs:\n",
    "        - df_reviews: DataFrame containing reviews. \n",
    "\n",
    "    Outputs:\n",
    "        - A list of chunked documents where each chunk is a sentence, represented as a Document object \n",
    "          with its associated metadata (e.g., document ID).\n",
    "    \"\"\"\n",
    "    from nltk.tokenize import sent_tokenize  # Import sentence tokenizer from NLTK.\n",
    "\n",
    "    # Initialize a list to store sentence-level Document objects.\n",
    "    doc_chunks = []\n",
    "\n",
    "    # Iterate through each row in the input DataFrame to process the reviews.\n",
    "    for index, row in df_reviews.iterrows():\n",
    "        # Extract the review text from the \"Doc Text\" column.\n",
    "        doc_text = row[\"Doc Text\"]\n",
    "\n",
    "        # Tokenize the review text into sentences.\n",
    "        sentences = sent_tokenize(doc_text)\n",
    "\n",
    "        # Create metadata for the document using the \"Doc Id\" column.\n",
    "        doc_id = {\"review_id\": row[\"Doc Id\"]}\n",
    "\n",
    "        # Iterate through each sentence to create sentence-level Document objects.\n",
    "        for sent in sentences:\n",
    "            # Create a new Document object with the sentence text and metadata.\n",
    "            newDoc = Document(page_content=sent, metadata=doc_id)\n",
    "\n",
    "            # Append the Document object to the doc_chunks list.\n",
    "            doc_chunks.append(newDoc)\n",
    "\n",
    "    # Return the list of sentence-level chunked documents.\n",
    "    return doc_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67ef7e1-3c4f-457c-a42d-cee1b0c8d96b",
   "metadata": {},
   "source": [
    "### Perform chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fd82473",
   "metadata": {},
   "outputs": [],
   "source": [
    "if chunking_strategy== \"passage_chunking\":\n",
    "    docs=passage_chunking(df_reviews, size=200, overlap=20)\n",
    "elif chunking_strategy== \"sentence_chunking\":\n",
    "    docs=sentence_chunking(df_reviews)\n",
    "else:\n",
    "    docs=None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcb1a14-a4c7-45f4-9209-0df1dda62b6a",
   "metadata": {},
   "source": [
    "### Print example chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02528c5e-ca07-4df9-a6a4-8e214e1bde16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Id: 40901\n",
      "\n",
      "Generated chunks:\n",
      "• Now I know why Guy featured this place, it was awesome!  Totally home cooked and authentic, and I should know!  Even better than grandma's food (God rest her soul). The posole was awesome, the\n",
      "• was awesome, the carnitas great, and the tacos are huge.  Order just one and you'll be satisfied, two and your stuffed!  Can't wait to go back next time I'm in Santa Barbara.  The place isn't exactly\n",
      "• place isn't exactly upscale or great for a first date, or if you're a snob.  But if you want some great Mexican food, that is home cooked and authentic, this is the place.  I'd also try the carne\n",
      "• also try the carne asada torta (like a steak sandwich), it looked great.\n",
      "\n",
      "\n",
      "Full review text:\n",
      "Now I know why Guy featured this place, it was awesome!  Totally home cooked and authentic, and I should know!  Even better than grandma's food (God rest her soul). The posole was awesome, the carnitas great, and the tacos are huge.  Order just one and you'll be satisfied, two and your stuffed!  Can't wait to go back next time I'm in Santa Barbara.  The place isn't exactly upscale or great for a first date, or if you're a snob.  But if you want some great Mexican food, that is home cooked and authentic, this is the place.  I'd also try the carne asada torta (like a steak sandwich), it looked great.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_ids= list(df_reviews[\"Doc Id\"].unique())\n",
    "doc_id=doc_ids[2]\n",
    "print(\"Review Id:\", doc_id)\n",
    "print(\"\\nGenerated chunks:\")\n",
    "\n",
    "example_chunks=[d for d in docs if d.metadata[\"review_id\"]==doc_id]\n",
    "for chunk in example_chunks:\n",
    "    print(\"\\u2022 \"+ chunk.page_content)\n",
    "\n",
    "print(\"\\n\")\n",
    "review_text= df_reviews[df_reviews[\"Doc Id\"]==doc_id][\"Doc Text\"].values[0]\n",
    "print(\"Full review text:\\n\"+ review_text)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69035807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 69890\n",
      "Average number of words per chunk: 28.15\n"
     ]
    }
   ],
   "source": [
    "def average_words_per_string(list_of_strings):\n",
    "    total_words = 0\n",
    "    total_strings = len(list_of_strings)\n",
    "    \n",
    "    for string in list_of_strings:\n",
    "        total_words += len(string.split())\n",
    "    \n",
    "    if total_strings == 0:\n",
    "        return 0  \n",
    "    \n",
    "    return total_words / total_strings\n",
    "    \n",
    "l=[doc.page_content for doc in docs]\n",
    "print(\"Number of chunks:\", len(l))\n",
    "average = average_words_per_string(l)\n",
    "print(\"Average number of words per chunk:\",f\"{average:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6977926c",
   "metadata": {},
   "source": [
    "###  Select embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c651d817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_221520/1721562529.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_function = SentenceTransformerEmbeddings(model_name=embedd_model)\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "embedd_model=\"all-MiniLM-L6-v2\" # all-mpnet-base-v2\"\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=embedd_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad9a73e",
   "metadata": {},
   "source": [
    "### Save embedding vectors to local vector database using FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d9a6766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "faiss = FAISS.from_documents(docs, embedding_function)\n",
    "save_to=\"data/Embeddings/\" + dataset + \"_\" + chunking_strategy\n",
    "faiss.save_local(save_to, index_name=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f870d97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
