{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "738c5cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emilh/anaconda3/envs/ou/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain import FAISS\n",
    "import os, sys\n",
    "from txtai.embeddings import Embeddings\n",
    "from langchain.docstore.document import Document \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import openai\n",
    "import re\n",
    "import json\n",
    "from nltk import tokenize\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fbad6c-6d3f-4bee-8eac-5bbea1eb0572",
   "metadata": {},
   "source": [
    "# Guide to Opinion Units\n",
    "- This guide shows how to create Opinion Units from a subset of Yelp restaurant reviews using the OpenAI LLM API. It further covers embedding the opinion units using SentenceTransformers and LangChain Documents and saving them to a local vector database file using FAISS.\n",
    "- To create Opinion Units using the OpenAI-LLM-API you need to create your own API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fe4179",
   "metadata": {},
   "source": [
    "### Read a subset of YELP restaurant review data (20k reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2839e3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=\"YELP\"\n",
    "data_path = 'data/YELP/yelp_subset.pkl'\n",
    "# Load a DataFrame of a subset of 20k YELP restaurant reviews from a pickle file\n",
    "df_reviews = pd.read_pickle(data_path)\n",
    "# Reset the index and rename the index column to \"Doc Id\"\n",
    "df_reviews.reset_index(inplace=True)\n",
    "df_reviews.rename(columns={'index': 'Doc Id'}, inplace=True)\n",
    "# Rename the column from 'text' to 'Doc Text'\n",
    "df_reviews.rename(columns={'text': 'Doc Text'}, inplace=True)\n",
    "columns_to_keep = ['Doc Id', 'review_id',\"business_id\",\"stars\",\"Doc Text\"]\n",
    "# Keep only the columns in the list\n",
    "df_reviews = df_reviews[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0553d89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>Doc Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112388</td>\n",
       "      <td>vhETeXa3nM34Hwk3KEFfiA</td>\n",
       "      <td>AQw0B8j9QV1RkFLLFiwkuw</td>\n",
       "      <td>3.0</td>\n",
       "      <td>I will be spending several weekends here in Ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68092</td>\n",
       "      <td>M09LOjNR1ymX4avcBQfAYQ</td>\n",
       "      <td>rh6O8NtKJUhqZ0G2Pkpj2Q</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Went here once and can't wait to go again! The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40901</td>\n",
       "      <td>w5x1pXvmODU5cYI3PZsSQA</td>\n",
       "      <td>YGgGefpPTFhgthvQvMAGoQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Now I know why Guy featured this place, it was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19599</td>\n",
       "      <td>LnbFwaD8CEC-OsCMb1YZDA</td>\n",
       "      <td>SZU9c8V2GuREDN5KgyHFJw</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Great place at the end of the wharf. Be prepar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>144853</td>\n",
       "      <td>3ZiPH6CHL_cyVNoYP2rt1Q</td>\n",
       "      <td>FQxEfhBd1gMrurP19bhK8w</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Mmm...I always get the chicken salad sandwich,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Doc Id               review_id             business_id  stars  \\\n",
       "0  112388  vhETeXa3nM34Hwk3KEFfiA  AQw0B8j9QV1RkFLLFiwkuw    3.0   \n",
       "1   68092  M09LOjNR1ymX4avcBQfAYQ  rh6O8NtKJUhqZ0G2Pkpj2Q    5.0   \n",
       "2   40901  w5x1pXvmODU5cYI3PZsSQA  YGgGefpPTFhgthvQvMAGoQ    5.0   \n",
       "3   19599  LnbFwaD8CEC-OsCMb1YZDA  SZU9c8V2GuREDN5KgyHFJw    4.0   \n",
       "4  144853  3ZiPH6CHL_cyVNoYP2rt1Q  FQxEfhBd1gMrurP19bhK8w    4.0   \n",
       "\n",
       "                                            Doc Text  \n",
       "0  I will be spending several weekends here in Ca...  \n",
       "1  Went here once and can't wait to go again! The...  \n",
       "2  Now I know why Guy featured this place, it was...  \n",
       "3  Great place at the end of the wharf. Be prepar...  \n",
       "4  Mmm...I always get the chicken salad sandwich,...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c806c65-135e-48d4-9648-f541fed03735",
   "metadata": {},
   "source": [
    "### Create query (prompt) template for prompting the LLM\n",
    "- The query template consists of: query_string = instructions for creating opinion units + example review + example response from_the LLM \n",
    "- info on OpenAI API with python: https://gokhang1327.medium.com/getting-started-with-the-openai-api-chatgpt-in-python-d689eecbbd37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5575271",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions1=\"Perform aspect-based sentiment analysis for the restaurant review provided as the input. Return each aspect-sentiment pair with a label and a corresponding excerpt from the text. Also mark the sentiment of aspects as negative or positive.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33a8bf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions2=\"Aspect-sentiment pairs should not mix opinions on different aspects. Make sure to include all aspects. An aspect should be independent and not have to rely on other aspects to be understood.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "336c8756",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions3=\"If an opinion in the review is about the restaurant or experience in general then label this aspect as “overall experience”.  Opinions not related to the restaurant should not be included.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8826bbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input= \"I just left Mary’s with my lovely wife. The gorgeous outdoor patio seating was fantastic with a nice view of the ocean. We came for brunch and were blown away! We split a dozen oysters. They were the best I had in my life! FRESH! Delicious! The avocado toast was excellent as were the crab cakes. Altogether, we had a great experience. Almost 5 stars! but the staff could have been a little friendlier and the tables cleaner.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20dd29ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_output=\"\"\"[[\"Outdoor patio seating\", \"The gorgeous outdoor patio seating was fantastic with a nice view of the ocean\", \"positive\"], \n",
    "[\"View\", \"a nice view of the ocean\", \"positive\"],\n",
    " [\"Brunch\", \"We came for brunch and were blown away\", \"positive\"], \n",
    "[\"Oysters\", \"We split a dozen oysters. They were the best I had in my life! FRESH! Delicious!\", \"positive\"], \n",
    "[\"Avocado toast\", \"the avocado toast was excellent\", \"positive\"], \n",
    "[\"Crab cakes\", \"the crab cakes were excellent\", \"positive\"],\n",
    "[\"Overall experience\", \"Altogether, we had a great experience. Almost 5 stars!\", \"positive\"], \n",
    "[\"Staff friendliness\", \"the staff could have been a little friendlier\", \"negative\"], \n",
    "[\"Table cleanliness\", \"the tables could have been cleaner\", \"negative\"]]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ebaad4",
   "metadata": {},
   "source": [
    "### Read your OpenAI API key \n",
    "#### !!! You need to have your own OPENAI_API_KEY !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5a609eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key (masked): sk-................................................................................................................................................\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from key_file import OPENAI_API_KEY\n",
    "OpenAI.api_key = OPENAI_API_KEY\n",
    "\n",
    "# Check if API key is set\n",
    "if OpenAI.api_key is None:\n",
    "    print(\"Error: OpenAI API key not found. Please set the environment variable 'OPENAI_API_KEY' or import from a secure file.\")\n",
    "else:\n",
    "    print(\"OpenAI API key (masked):\", OpenAI.api_key[:3] + \"...\" * (len(OpenAI.api_key) - 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b8670c",
   "metadata": {},
   "source": [
    "### Functions used to create Opinion units and to check their format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fd79689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query_string(instructions1, instructions2, instructions3, example_input, example_output, review_input):\n",
    "    \"\"\"\n",
    "    Function aim: creates the query_string (text in prompt template) sent to the LLM-API\n",
    "    \n",
    "    query_string = instructions + review_query + example_query + example_answer \n",
    "    \"\"\"\n",
    "    query_string = \"\"\"{}\n",
    "\n",
    "{}\n",
    "\n",
    "{}\n",
    "\n",
    "Example input: {}\n",
    "\n",
    "Example output: \n",
    "{}\n",
    "\n",
    "Input: {}\n",
    "\n",
    "Output: \"\"\".format(instructions1, instructions2, instructions3,\n",
    "               example_input, example_output, review_input)\n",
    "    return query_string\n",
    "\n",
    "def create_opinion_units(query_string):\n",
    "    \"\"\"\n",
    "    Function aim: Call LLM API and generate opinion units\n",
    "    \n",
    "    Input: query_string (prompt template text)  \n",
    "    \"\"\"\n",
    "    # Import the OpenAI library and initialize the client with the API key.\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    \n",
    "    # Generate a completion using the OpenAI chat model.\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\", # Specify the model to use (GPT-4 Turbo).\n",
    "        #model=\"gpt-3.5-turbo\",  # Specify the model to use (GPT-3.5 Turbo).\n",
    "        messages=[  # Define the messages to be used for generating the response.\n",
    "            {\"role\": \"user\", \"content\": query_string}  # User's message.\n",
    "        ],\n",
    "        max_tokens=2400,  # Maximum number of tokens in the response.\n",
    "        temperature=1.0  # Sampling temperature, controlling the randomness of the response.\n",
    "    )\n",
    "    \n",
    "    # Return the generated completion.\n",
    "    return completion\n",
    "\n",
    "def check_and_parse_ous(data):\n",
    "    \"\"\"\n",
    "    This function has two functions:\n",
    "    1. Check that format corresponds with e.g. \n",
    "    \n",
    "        [[\"Avocado toast\", \"the avocado toast was excellent\", \"positive\"], \n",
    "        [\"Crab cakes\", \"the crab cakes were excellent\", \"positive\"]]\n",
    "    \n",
    "    2. Disregards potential other text in LLM response except for opinion unit-json\n",
    "\n",
    "    Input: Raw text response from LLM\n",
    "    Output: \n",
    "        If correct format: True (correct format), ou_df (dataframe storing opinin unit) \n",
    "        If incorrect format: Fakse (incorrect format), False\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Regular expression to find the list in the string\n",
    "        match = re.search(r'\\[\\[.*?\\]\\]', data, re.DOTALL)\n",
    "\n",
    "        if match:\n",
    "            # Extract the list string\n",
    "            data = match.group(0)\n",
    "        else:\n",
    "            return False, False\n",
    "        \n",
    "        # Load the data to check if it's a valid JSON array\n",
    "        parsed_data = json.loads(data)\n",
    "\n",
    "        # Check if the outer structure is a list\n",
    "        if not isinstance(parsed_data, list):\n",
    "            return False, False\n",
    "\n",
    "        # Check each element in the list\n",
    "        for item in parsed_data:\n",
    "            # Each item should be a list with exactly 3 elements\n",
    "            if not (isinstance(item, list) and len(item) == 3):\n",
    "                return False, False\n",
    "            # Check that the first two elements are strings and the third is a valid sentiment\n",
    "            if not (isinstance(item[0], str) and isinstance(item[1], str)):\n",
    "                return False, False\n",
    "        \n",
    "        ou_df = pd.DataFrame(parsed_data, columns=['Aspect', 'Extract', 'Sentiment'])\n",
    "        \n",
    "        return True, ou_df\n",
    "    except json.JSONDecodeError:\n",
    "        False, False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f814984c-4bee-4c6b-9d0f-3a2848905019",
   "metadata": {},
   "source": [
    "### Optional subsampling of dataset or selection of specific reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e09491ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numer of reviews for opinion unit generation: 3\n"
     ]
    }
   ],
   "source": [
    "# only use X reviews during experimentation\n",
    "df_reviews=df_reviews.sample(n=3, random_state=23)\n",
    "# sample specific reviews from dataset based on \"Doc Id\"\n",
    "#specific_rows = error_ids\n",
    "#df_reviews=df_reviews[df_reviews[\"Doc Id\"].isin(specific_rows)]\n",
    "print(\"Numer of reviews for opinion unit generation:\", len(df_reviews.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb4b36c",
   "metadata": {},
   "source": [
    "### Loop through reviews and create opinion units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b9128a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating opinion units...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating opinion-units: 100%|█████████████████████| 3/3 [00:18<00:00,  6.33s/it]\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store Document objects of created option units\n",
    "docs = []\n",
    "# Doc Ids of Opinion Units with incorrect format i.e. our LLM returned the wrong format \n",
    "error_ids=[]\n",
    "\n",
    "#number of reviews\n",
    "total_texts=len(df_reviews.index)\n",
    "counter=0\n",
    "\n",
    "print(\"Creating opinion units...\")\n",
    "# tqdm (pbar) creates a progress bar for us\n",
    "with tqdm(total=len(df_reviews.index), desc=\"Creating opinion-units\") as pbar:\n",
    "    for index, review in df_reviews.iterrows():\n",
    "        # update number of completed reviews in progress bar\n",
    "        pbar.update(1)\n",
    "        # Extract the text of the review\n",
    "        review_input=review[\"Doc Text\"]\n",
    "\n",
    "        # create query_string (sent as query/prompt to LLM)\n",
    "        query_string=create_query_string(instructions1, instructions2, instructions3,\n",
    "                                         example_input, example_output, review_input)\n",
    "\n",
    "        # Use LLM-gpt API to create prepositions\n",
    "        chat_completion_object = create_opinion_units(query_string)\n",
    "\n",
    "        # Extract the opinion_units from the completion object\n",
    "        ous_string = chat_completion_object.choices[0].message.content\n",
    "\n",
    "        # check if the LLM returned the correct Opinion Unit format, if incorrect format: ou_check = False\n",
    "        ou_check, ous_df=check_and_parse_ous(ous_string)\n",
    "        if not ou_check:\n",
    "            print(\"Error in format for review ID:\",review[\"Doc Id\"])\n",
    "            error_ids.append(review[\"Doc Id\"])\n",
    "            continue\n",
    "        else:\n",
    "            # Regular expression to find the list in the string\n",
    "            import re\n",
    "            # remove text from response that is not list\n",
    "            ous_string = re.search(r'\\[\\[.*?\\]\\]', ous_string, re.DOTALL).group(0)\n",
    "\n",
    "            # Iterate over each row (opinion unit) in the DataFrame ous_df\n",
    "            for ind, ou in ous_df.iterrows():\n",
    "\n",
    "                #ou_text is the form of the opinion unit minus the sentiment and Doc ID metadata\n",
    "                ou_text=ou[\"Aspect\"]+\": \"+ou[\"Extract\"]\n",
    "\n",
    "                # Create metadata dictionary containing review ID, sentiment, and aspec\n",
    "                meta_dict={\"review_id\":review[\"Doc Id\"], \"sentiment\":ou[\"Sentiment\"], \"Aspect\":ou[\"Aspect\"]}\n",
    "\n",
    "                # Create a new Document object with the opinion unit excerpt text and metadata\n",
    "                newDoc = Document(page_content=ou_text, metadata=meta_dict)\n",
    "                docs.append(newDoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9f033d",
   "metadata": {},
   "source": [
    "### Example output (opinion units) returned from LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f713781-f515-44b6-a1ae-cecbff02735e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Id: 8492\n",
      "\n",
      "Generated opinion units:\n",
      "• Food presentation: I was happily surprised at the set up of the plate. They separated the meat and cheese into a pile and it was up to you to assemble {sentiment: positive}\n",
      "• Quantity of food: It was ALOT of food for a very decent price {sentiment: positive}\n",
      "• Price value: a very decent price {sentiment: positive}\n",
      "• Overall experience: Will be back for round 2 real soon {sentiment: positive}\n",
      "\n",
      "\n",
      "Full review text:\n",
      "I had meant to take a pic of the food I ate, but when my number was called I just went into auto and demolished the plate. I was happily surprised at the set up of he plate. They separated the meat and cheese into a pile and it was up to you to assemble... It was ALOT of food for a very decent price. Will be back for round 2 real soon.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_ids= list(df_reviews[\"Doc Id\"].unique())\n",
    "doc_id=doc_ids[2]\n",
    "print(\"Review Id:\", doc_id)\n",
    "print(\"\\nGenerated opinion units:\")\n",
    "\n",
    "example_ous=[d for d in docs if d.metadata[\"review_id\"]==doc_id]\n",
    "for ou in example_ous:\n",
    "    print(\"\\u2022 \"+ ou.page_content+ \" {sentiment: \"+ou.metadata[\"sentiment\"]+\"}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "review_text= df_reviews[df_reviews[\"Doc Id\"]==doc_id][\"Doc Text\"].values[0]\n",
    "print(\"Full review text:\\n\"+ review_text)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881899dd",
   "metadata": {},
   "source": [
    "###  Select embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc9e3a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_214363/3997311824.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_function = SentenceTransformerEmbeddings(model_name=embedd_model)\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "embedd_model=\"all-MiniLM-L6-v2\"\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=embedd_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd3ea61",
   "metadata": {},
   "source": [
    "### Save embedding vectors to local vector database using FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c80ea829",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss = FAISS.from_documents(docs, embedding_function)\n",
    "chunking_strategy=\"opinion_units\"\n",
    "save_to=\"data/Embeddings/\" + dataset + \"_\" + chunking_strategy\n",
    "faiss.save_local(save_to, index_name=\"index\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
